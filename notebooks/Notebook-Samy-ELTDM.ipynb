{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation et distribution sous Spark d'un algorithme d'apprentissage non-supervisé : k-means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des librairies nécessaires et mise en place du contexte distribué Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from math import sqrt,log\n",
    "from random import randint, random, randrange,uniform\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sc.stop()\n",
    "\n",
    "conf = SparkConf().setAppName(\"kmeans\").setMaster(\"local[*]\")\n",
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Récupération du jeu de données s1.txt, à partir des S-sets : https://cs.joensuu.fi/sipu/datasets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s1 = sc.textFile(\"/Users/sam/s1.txt\")\n",
    "\n",
    "#Nous donnons une structure de liste particulière à notre jeu de données\n",
    "\n",
    "def extract_split(x):\n",
    "    splits = x.split('    ')\n",
    "    return (int(splits[1]), int(splits[2]))\n",
    "\n",
    "s1 = s1.flatMap(lambda x: x.split('\\n')).map(extract_split)\n",
    "\n",
    "s1 = s1.sample(withReplacement=False,fraction=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Pour visualiser cette structure particulière\n",
    "\n",
    "s1.takeSample(False, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialisation de l'algorithme k-means++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comme nous sommes dans un contexte distribué Spark, nous sommes contraints de redéfinir certaines fonctions, comme par exemple la fonction distance euclidienne (relative à la norme euclidienne usuelle).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Nous ajoutons une nouvelle colonne qui répertoriera les distances entre chaque point et chaque centre.\n",
    "\n",
    "def compute_distance(xi_indexes, yi_indexes):\n",
    "    def map(row):\n",
    "        sum = 0\n",
    "        for i in range(len(xi_indexes)):\n",
    "            sum += (row[yi_indexes[i]] - row[xi_indexes[i]]) ** 2\n",
    "        distance = sqrt(sum)\n",
    "        return row + (distance,)\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***L'algorithme k-means++ demande de tirer un premier centre de manière uniformément aléatoire parmi l'ensemble des points de données. Pour le reste des centres, il faudra :***\n",
    "\n",
    "***- calculer la distance entre chaque point xi et le centre déjà établi le plus proche***\n",
    "\n",
    "***- associer à chaque point xi une probabilité proportionnelle au carré de cette distance (par exemple le carré de cette distance sur la somme des carrés des distances entre chaque point xj et le centre déjà établi le plus proche)***\n",
    "\n",
    "***- tirer un nouveau centre de manière uniformément aléatoire à l'aide du vecteur de probabilités constitué par l'ensemble des probabilités calculées dans la deuxième étape***\n",
    "\n",
    "***- réitérer les étapes 2 et 3 jusqu'à ce que k centres aient été choisis***\n",
    "\n",
    "***- enfin, procéder de la même manière que l'algorithme k-means classique pour ajuster les centres (calculs de barycentres) et repositionner les points qui seraient dans les mauvais clusters***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Cette approche probabiliste pose problème dans le cadre distribué, nous allons voir pourquoi dans la suite de ce Notebook. Nous allons donc, en plus de la stratégie probabiliste, adopter d'autres stratégies d'initialisation.***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1ère stratégie : admettons que nous ayons n centres déjà établis. Pour choisir le (n+1)ème centre, nous allons observer les distances de chaque point xi (qui n'est pas un centre déjà établi) aux centres déjà établis. Pour un point donné xi, nous prenons la distance minimale à un centre déjà établi (autrement dit le centre déjà établi le plus proche de xi). Une fois que nous aurons fait ceci pour tous les xi, nous prenons comme (n+1)ème centre le xi qui est le plus éloigné de son centre (déjà établi) le plus proche.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceMax(dist_indexes):\n",
    "    def reduce_custom(x1,x2):\n",
    "        '''\n",
    "        print(\"\\n\")\n",
    "        '''\n",
    "        dist_x1 = []\n",
    "        dist_x2 = []\n",
    "        for idx in dist_indexes:\n",
    "            dist_x1.append(x1[idx])\n",
    "            dist_x2.append(x2[idx])\n",
    "        \n",
    "        '''\n",
    "        print(\"------\")\n",
    "        \n",
    "        print(\"X1 :\", x1[0], x1[1])\n",
    "        print(\"Dist x1 : \",dist_x1)\n",
    "        '''\n",
    "        mindist_x1 =  min(dist_x1)\n",
    "        \n",
    "        '''\n",
    "        print(\"Min dist x1 :\", mindist_x1)\n",
    "        \n",
    "        print(\"X2 :\", x2[0], x2[1])\n",
    "        print(\"Dist x2 : \", dist_x2)\n",
    "        '''\n",
    "        mindist_x2 = min(dist_x2)\n",
    "        '''\n",
    "        print(\"Min dist x2 :\", mindist_x2)\n",
    "        print(\"------\")\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        result = None\n",
    "        if(mindist_x1 > mindist_x2):\n",
    "            result = x1\n",
    "        else:\n",
    "            result = x2\n",
    "        \n",
    "        '''\n",
    "        print(\"Result :\", result[0], result[1])\n",
    "        \n",
    "        '''\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    return reduce_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2ème stratégie : admettons que nous ayons n centres déjà établis. Pour choisir le (n+1)ème centre, nous allons observer les distances de chaque point xi (qui n'est pas un centre déjà établi) aux centres déjà établis. Pour un point donné xi, nous sommons ses distances aux n centres déjà établis. Une fois que nous aurons fait ceci pour tous les xi, nous prenons comme (n+1)ème centre le xi dont la somme de ses distances aux n centres déjà établis est la plus grande.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceMaxSum(dist_indexes):\n",
    "    def reduce_custom(x1,x2):\n",
    "        dist_x1 = []\n",
    "        dist_x2 = []\n",
    "        for idx in dist_indexes:\n",
    "            dist_x1.append(x1[idx])\n",
    "            dist_x2.append(x2[idx])\n",
    "        \n",
    "        mindist_x1 =  sum(dist_x1)\n",
    "        mindist_x2 = sum(dist_x2)\n",
    "        \n",
    "        \n",
    "        if(mindist_x1 > mindist_x2):\n",
    "            return x1\n",
    "        else:\n",
    "            return x2\n",
    "        \n",
    "        \n",
    "    return reduce_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3ème stratégie : il s'agit de faire la même chose que la 2ème stratégie, sauf que pour chaque xi, la somme de ses distances aux n centres déjà établis sera pondérée par l'inverse du nombre de distances calculées pour xi.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceMaxAverage(dist_indexes):\n",
    "    def reduce_custom(x1,x2):\n",
    "        dist_x1 = []\n",
    "        dist_x2 = []\n",
    "        for idx in dist_indexes:\n",
    "            dist_x1.append(x1[idx])\n",
    "            dist_x2.append(x2[idx])\n",
    "        \n",
    "        mindist_x1 =  sum(dist_x1)/len(dist_x1)\n",
    "        mindist_x2 = sum(dist_x2)/len(dist_x2)\n",
    "        \n",
    "        \n",
    "        if(mindist_x1 > mindist_x2):\n",
    "            return x1\n",
    "        else:\n",
    "            return x2\n",
    "        \n",
    "        \n",
    "    return reduce_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4ème stratégie : il s'agit de la stratégie traditionnelle probabiliste de k-means++**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapDistance(dist_indexes):\n",
    "    def custom_map(row):\n",
    "        index = row[0]\n",
    "        table = row[1]\n",
    "        dists = []\n",
    "        for idx in dist_indexes:\n",
    "            dists.append(table[idx])\n",
    "\n",
    "        min_dist = min(dists)\n",
    "\n",
    "        return (index, min_dist)\n",
    "    return custom_map\n",
    "\n",
    "def basicMaxReduce(x1,x2):\n",
    "    return max(x1,x2)\n",
    "\n",
    "\n",
    "def findNextCenter(data, dist_indexes, coord_indexes):\n",
    "    dataZip = data.zipWithIndex().map(lambda x : (x[1],x[0]))\n",
    "    print (\"Data zip\", dataZip.take(3))\n",
    "    \n",
    "    #Map Min Dist\n",
    "    dataZipMap = dataZip.map(mapDistance(dist_indexes))\n",
    "    print (\"Data zip map\", dataZipMap.take(3))\n",
    "\n",
    "    #Normalize distance\n",
    "    nc = dataZipMap.map(lambda x: x[1])\n",
    "    \n",
    "    print (\"Data to extract max dist\", nc.take(3))\n",
    "    \n",
    "    nc = nc.reduce(lambda a,b:a+b)\n",
    "    \n",
    "    \n",
    "    print(\"Full cumulative sum is  : \", nc)\n",
    "    \n",
    "    dataZipMap = dataZipMap.map(lambda x: (x[0], x[1]/nc) )\n",
    "    \n",
    "    print(\"Data zip map normalized \", dataZipMap.take(3))\n",
    "    \n",
    "    collection = dataZipMap.collect()\n",
    "    print(collection[:3])\n",
    "    \n",
    "    #Draw best point\n",
    "    indexes = [item[0] for item in collection]\n",
    "    prob = [item[1] for item in collection]\n",
    "    \n",
    "    print(indexes[:3])\n",
    "    print(prob[:3])\n",
    "    print(sum(prob))\n",
    "    \n",
    "    cumsum = np.cumsum(prob)\n",
    "    \n",
    "    index_best_point = None\n",
    "    \n",
    "    draw = uniform(0,1)\n",
    "    print(\"Draw : \", draw)\n",
    "    print(\"Cumsum 0\", cumsum[0])\n",
    "    if(draw < cumsum[0]):\n",
    "        index_best_point = indexes[0]\n",
    "    else:\n",
    "        current_index = 1\n",
    "        current_sum = cumsum[current_index]\n",
    "        previous_sum = cumsum[current_index -1]\n",
    "        while not(draw < current_sum and draw >= previous_sum):\n",
    "            #print(\"Index loop \", current_index)\n",
    "            #print(\"Current sum \", current_sum)\n",
    "            #print(\"Previous sum\", previous_sum)\n",
    "            \n",
    "            current_index += 1\n",
    "            previous_sum = cumsum[current_index -1]\n",
    "            current_sum = cumsum[current_index]\n",
    "            \n",
    "        index_best_point = current_index\n",
    "    \n",
    "    best_point = dataZip.filter(lambda x: x[0] == index_best_point).collect()\n",
    "    print(\"Best point \", best_point)\n",
    "    result = tuple([best_point[0][1][coord] for coord in coord_indexes])\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5ème stratégie : tentative de distribution d'un algorithme probabiliste. L'algorithme est une sorte de tournoi entre les points. Nous avons deux points avec pour chacun  d'entre eux, la distance à leur cluster le plus proche, notées d1 et d2. Nous tirons une loi uniforme sur le compact [0;d1+d2]. Notre règle de décision est la suivante : nous choisissons X1 si U < D1, et nous choisissons X2 sinon. Ainsi, nous réalisons des tournois 1 contre 1 (un tournoi entre deux points), ce qui nous donne de multiples réalisations de lois uniformes en 1 contre 1.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceMaxWithRandom(dist_indexes):\n",
    "    def reduce_custom(x1,x2):\n",
    "        dist_x1 = []\n",
    "        dist_x2 = []\n",
    "        for idx in dist_indexes:\n",
    "            dist_x1.append(x1[idx])\n",
    "            dist_x2.append(x2[idx])\n",
    "        \n",
    "        mindist_x1 =  min(dist_x1)\n",
    "        mindist_x2 = min(dist_x2)\n",
    "        \n",
    "        \n",
    "        draw = uniform(0,mindist_x1 + mindist_x2)    \n",
    "        \n",
    "        '''\n",
    "        print(\"\\n\")\n",
    "        print(\"X1 :\", x1[0], x1[1])\n",
    "        print(\"X2 :\", x2[0], x2[1])\n",
    "        \n",
    "        print(\"Min_dist x1 :\", mindist_x1)\n",
    "        print(\"Min_dist x2 :\", mindist_x2)\n",
    "        print(\"Draw : \", draw)\n",
    "        '''\n",
    "        result = x1\n",
    "        if(draw > mindist_x1):\n",
    "            result = x2\n",
    "            \n",
    "        #print (\"Result : \",result[0], result[1])\n",
    "        return result\n",
    "        \n",
    "            \n",
    "    return reduce_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6ème stratégie : même stratégie que la 2ème, sauf que notre règle de décision ne sera pas basée sur le maximum entre deux sommes de distances minimales, mais plutôt entre la somme des distances minimales pour X1 et une valeur tirée selon une loi uniforme sur le compact [0 ; somme des distances minimales pour X1 + somme des distances minimales pour X2] (remarque : l'algorithme fait le traitement pour tous les Xi, il ne choisit pas X1 en particulier ; nous avons mentionné X1 ici pour expliquer comment l'algorithme fonctionne sur deux points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduceMaxWithRandomSum(dist_indexes):\n",
    "    def reduce_custom(x1,x2):\n",
    "        dist_x1 = []\n",
    "        dist_x2 = []\n",
    "        for idx in dist_indexes:\n",
    "            dist_x1.append(x1[idx])\n",
    "            dist_x2.append(x2[idx])\n",
    "        \n",
    "        mindist_x1 =  sum(dist_x1)\n",
    "        mindist_x2 = sum(dist_x2)\n",
    "        \n",
    "        \n",
    "        draw = uniform(0,mindist_x1 + mindist_x2)    \n",
    "            \n",
    "        result = x1\n",
    "        if(draw > mindist_x1):\n",
    "            result = x2\n",
    "        return result\n",
    "        \n",
    "            \n",
    "    return reduce_custom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les trois algorithmes qui suivent seront utiles à la deuxième phase de l'algorithme k-means++, identique à la deuxième phase de l'algorithme kmeans (une fois l'initialisation terminée)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Réajustement des centres : calculs des barycentres de chaque cluster***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_average(list_reduce, coord_indexes):\n",
    "    result = ()\n",
    "    for idx in coord_indexes:\n",
    "        coord_list = [reduce_tuple[1][idx] for reduce_tuple in list_reduce]\n",
    "        average = sum(coord_list)/len(coord_list)\n",
    "        result += (average,)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Représentation graphique des clusters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def drawGraph(data, cluster_centers):\n",
    "    datalist = data.collect()\n",
    "    plt.figure()\n",
    "    x = [point[0] for point in datalist]\n",
    "    y = [point[1] for point in datalist]\n",
    "    \n",
    "    x_clusters = [point[0] for point in cluster_centers]\n",
    "    y_clusters = [point[1] for point in cluster_centers]\n",
    "    \n",
    "    plt.plot(x,y,'bs', x_clusters, y_clusters, 'r^')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mapAddTuple(t, coord_idx):\n",
    "    def map(row):\n",
    "        \n",
    "        toAdd = tuple([deepcopy(t[i]) for i in coord_idx])\n",
    "        \n",
    "        return row + toAdd\n",
    "    return map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme d'initialisation kmeans++ incluant les algorithmes précédents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initCluster(data,num_clusters,num_features, reducer, cluster_centers, visualize=False, distributed = True):\n",
    "    xi_indexes = [idx for idx in range(0,num_features)]\n",
    "    yi_indexes = [idx for idx in range(num_features, num_features*2)]\n",
    "    coord_indexes = [idx for idx in range(num_features)]\n",
    "    dist_indexes = [num_features*2]\n",
    "    current_clust = 2\n",
    "    \n",
    "    '''\n",
    "    print(\"Initial xi_indexes : \", xi_indexes)\n",
    "    print(\"Initial yi_indexes : \", yi_indexes)\n",
    "    print(\"Initial coord_indexes : \",coord_indexes)\n",
    "    print(\"Initial dist_indexes : \", dist_indexes)\n",
    "    print(\"Intial current_clust :\", current_clust)\n",
    "    print(\"\\n\")\n",
    "    '''\n",
    "    \n",
    "    for _ in range(num_clusters-1):\n",
    "        \n",
    "        #print(\"#############################\")\n",
    "        \n",
    "        \n",
    "        print(\"Current cluster points :\", cluster_centers)\n",
    "        '''\n",
    "        print(\"Data before adding distance\")\n",
    "        collect = data.collect()\n",
    "        for line in data.collect():\n",
    "            print(line, \"\\n\")\n",
    "        '''\n",
    "        \n",
    "        data_dist = data.map(compute_distance(xi_indexes, yi_indexes))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        if not distributed:\n",
    "            new_cluster_point = findNextCenter(coord_indexes=coord_indexes,data=data_dist,dist_indexes=dist_indexes)\n",
    "        else:\n",
    "            '''\n",
    "            print(\"\\n Data with distance\")\n",
    "            for line in data_dist.collect():\n",
    "                print(line, \"\\n\")\n",
    "\n",
    "            '''\n",
    "            reduce_tuple = data_dist.reduce(reducer(dist_indexes=dist_indexes))\n",
    "\n",
    "            ''' \n",
    "            print(\"\\n Data after reduce\")\n",
    "            for line in data_dist.collect():\n",
    "                print(line, \"\\n\")\n",
    "            '''\n",
    "        \n",
    "        \n",
    "            new_cluster_point = tuple(reduce_tuple[i] for i in coord_indexes)\n",
    "            \n",
    "        cluster_centers.append(new_cluster_point)\n",
    "        \n",
    "        \n",
    "        print(\"\\n Cluster list : \", cluster_centers)\n",
    "                                  \n",
    "        #print(\"Computing center of cluster n° {}\".format(current_clust))\n",
    "        print(\"\\n New cluster point : {}\".format(new_cluster_point))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#CE MAP ETAIT LA SOURCE DES SOUCI : EN EFFET, NOUS NOUS RETROUVIONS AVEC DES NOUVEAUX CENTRES QUI ECRASAIENT LES\n",
    "#ANCIENS DANS LE MAP, ET C'EST POUR CELA QUE NOUS NOUS RETROUVIONS AVEC UN NOMBRE DE CENTRES REDUIT (ET DE NOMBREUX\n",
    "#CENTRES IDENTIQUES) A L'ARRIVEE. AINSI, POUR PALLIER CE PROBLEME, NOUS AVONS ENCAPSULE LE CODE DANS UNE FONCTION.\n",
    "        \n",
    "        data_point = data_dist.map(mapAddTuple(new_cluster_point,coord_idx=coord_indexes))\n",
    "        \n",
    "        data = data_point\n",
    "        \n",
    "        data_point.count()\n",
    "        \n",
    "        '''\n",
    "        print(\"\\n Data after adding new cluster point\")\n",
    "        for line in data_point.collect():\n",
    "            print(line, \"\\n\")\n",
    "        '''\n",
    "        \n",
    "        if visualize:\n",
    "            #Assume that data is 2D, and coord is 0,1\n",
    "            drawGraph(data, cluster_centers)\n",
    "        \n",
    "        #Update of variables\n",
    "        current_clust += 1\n",
    "        dist_indexes.append(current_clust*num_features + current_clust - 2)\n",
    "        yi_indexes = [old_value + num_features + 1 for old_value in yi_indexes]\n",
    "        \n",
    "        #print(\"New yi indexes : {}\".format(yi_indexes))\n",
    "        #print(\"New dist_indexes : {}\".format(dist_indexes))\n",
    "        \n",
    "        '''\n",
    "        print(\"\\n\")\n",
    "        print(\"##############################\")\n",
    "        '''\n",
    "    return data, cluster_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de l'algorithme k-means++ sur nos données (version non distribuée et initialisation reduceMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-7140f77c3fb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster1_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms1_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcluster1_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms1_map_clustered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceMax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's1' is not defined"
     ]
    }
   ],
   "source": [
    "cluster1_center = s1.takeSample(False, 1)[0]\n",
    "cluster_centers = [cluster1_center]\n",
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "s1_map = s1.map(lambda x : x + cluster1_center)\n",
    "s1_map_clustered, cluster_centers = initCluster(s1_map, num_features=2,num_clusters=10, reducer=reduceMax, cluster_centers=cluster_centers, visualize=True, distributed=False)\n",
    "\n",
    "\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de l'algorithme k-means++ sur nos données (version distribuée et initialisation reduceMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-c63d7b9eaeba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster1_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0ms1_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcluster1_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms1_map_clustered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceMax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's1' is not defined"
     ]
    }
   ],
   "source": [
    "cluster1_center = s1.takeSample(False, 1)[0]\n",
    "cluster_centers = [cluster1_center]\n",
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "s1_map = s1.map(lambda x : x + cluster1_center)\n",
    "s1_map_clustered, cluster_centers = initCluster(s1_map, num_features=2,num_clusters=10, reducer=reduceMax, cluster_centers=cluster_centers, visualize=True)\n",
    "\n",
    "\n",
    "print(cluster_centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de l'algorithme k-means++ sur nos données (version distribuée et initialisation reduceMaxWithRandom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster1_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0403aeb8fda9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms1_map_clustered2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceMaxWithRandom\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcluster_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster1_center' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "cluster_centers = [cluster1_center]\n",
    "s1_map_clustered2 = initCluster(s1_map, num_features=2,num_clusters=10, reducer=reduceMaxWithRandom,  cluster_centers=cluster_centers, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de l'algorithme k-means++ sur nos données (version distribuée et initialisation reduceMaxSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster1_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-d08fa9eb6ed1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms1_map_clustered2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceMaxSum\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcluster_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster1_center' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "cluster_centers = [cluster1_center]\n",
    "s1_map_clustered2 = initCluster(s1_map, num_features=2,num_clusters=10, reducer=reduceMaxSum,  cluster_centers=cluster_centers, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de l'algorithme k-means++ sur nos données (version distribuée et initialisation reduceMaxAverage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster1_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-0d3f5d18fbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms1_map_clustered2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceMaxAverage\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcluster_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster1_center' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "cluster_centers = [cluster1_center]\n",
    "s1_map_clustered2 = initCluster(s1_map, num_features=2,num_clusters=10, reducer=reduceMaxAverage,  cluster_centers=cluster_centers, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation de l'algorithme k-means++ sur nos données (version distribuée et initialisation reduceMaxWithRandomSum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster1_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0a1e63396e1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0ms1_map_clustered2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitCluster\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreducer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreduceMaxWithRandomSum\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcluster_centers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster1_center' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "cluster_centers = [cluster1_center]\n",
    "s1_map_clustered2 = initCluster(s1_map, num_features=2,num_clusters=10, reducer=reduceMaxWithRandomSum,  cluster_centers=cluster_centers, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithme final k-means++ et visualisation toutes les dix itérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def distance(x1, x2):\n",
    "    sum = 0\n",
    "    for i in range(len(x1)):\n",
    "        sum += (x2[i] - x1[i]) ** 2\n",
    "    \n",
    "    distance_result = sqrt(sum)\n",
    "    \n",
    "    return distance_result\n",
    "\n",
    "\n",
    "#L X Y X1 Y1 D1 X2 Y2 D2 X3 Y3 D3\n",
    "#0 1 2 3  4  5  6  7  8  9  10 11\n",
    "def computeCluster(num_features, num_clusters):\n",
    "    def map_cluster(row):\n",
    "        \n",
    "        #print(\"====== \\n Map compute cluster distance\")\n",
    "        index_coord = [index for index in range(1,num_features + 1)]\n",
    "        #print(\"index coords :\", index_coord)\n",
    "        \n",
    "        coords_value = [row[index] for index in index_coord]\n",
    "            \n",
    "        indexes_clusters = [[index_cluster*num_features + index_features + index_cluster for index_features in range(0,num_features)] for index_cluster in range(1, num_clusters+ 1)]\n",
    "        #print(\"index centers initial : \", indexes_clusters)\n",
    "        \n",
    "        indexes_distance = [index_cluster*num_features + index_cluster - 1 for index_cluster in range(2, num_clusters+ 2)]\n",
    "        #print(\"indexes distance : \", indexes_distance)\n",
    "        \n",
    "        cluster_distances = []\n",
    "        \n",
    "        for cluster_coord_idx in indexes_clusters :\n",
    "            \n",
    "            coords_cluster = [row[index] for index in cluster_coord_idx]\n",
    "            \n",
    "            distance_cluster = distance(coords_value, coords_cluster)\n",
    "            #print(\"Coord cluster \", coords_cluster)\n",
    "            #print(\"Coord value \", coords_value)\n",
    "            #print(\"Distance : \", distance_cluster)\n",
    "            \n",
    "            cluster_distances.append(distance_cluster)\n",
    "            \n",
    "        #print(\"Cluster distance array \", cluster_distances)\n",
    "        \n",
    "        index_max = np.argmin(cluster_distances) + 1\n",
    "        #print(\"Index min : \", index_max)\n",
    "        row_list = list(row)\n",
    "        row_list[0] = index_max\n",
    "        \n",
    "        row = tuple(row_list)\n",
    "        \n",
    "        #print(\"========\")\n",
    "        \n",
    "        return row\n",
    "    return map_cluster\n",
    "    \n",
    "def formatDataReduce(num_features, num_clusters):\n",
    "    def map_format(row):\n",
    "        cluster = row[0]\n",
    "        #indexes_clusters = [cluster * num_features + index_features + cluster for index_features in range(0,num_features)]\n",
    "        indexes_coords = [index for index in range(1,num_features + 1)]\n",
    "        \n",
    "        coords_value = [row[index] for index in indexes_coords]\n",
    "        #coords_cluster = [row[index] for index in indexes_clusters]\n",
    "        \n",
    "        return (cluster, (coords_value,1))\n",
    "    return map_format\n",
    "\n",
    "def reduceComputeCenter(num_features):\n",
    "    def reduce_center(x1,x2):\n",
    "        \n",
    "        #print(\"X1 \", x1)\n",
    "        #print(\"X2\", x2)\n",
    "        \n",
    "        \n",
    "        coords_x1 = x1[0]\n",
    "        coords_x2 = x2[0]\n",
    "        \n",
    "        #print(\"Coords x1 \", coords_x1)\n",
    "        #print(\"Coords x2\", coords_x2)\n",
    "        coords_sum = [coords_x1[i] + coords_x2[i] for i in range(0, num_features)]\n",
    "        \n",
    "        #print(\"Sum of coords\", coords_sum)\n",
    "        \n",
    "        x1_num = x1[1]\n",
    "        x2_num = x2[1]\n",
    "        \n",
    "        return tuple([coords_sum, x1_num + x2_num])\n",
    "    return reduce_center\n",
    "\n",
    "def updateClusterCoords(num_features, num_clusters, reduce):\n",
    "    def map_update_cluster(row):\n",
    "        \n",
    "        #print(\"Reduce : \", reduce)\n",
    "        indexes_clusters = [[index_cluster*num_features + index_features + index_cluster for index_features in range(0,num_features)] for index_cluster in range(1, num_clusters+ 1)]\n",
    "        \n",
    "        #print(\"index centers : \", indexes_clusters)\n",
    "        \n",
    "        num_cluster_real = len(reduce)\n",
    "        for cluster in range(0,num_cluster_real):\n",
    "            new_coordinates = reduce[cluster][1]\n",
    "            cluster_center_indexes = indexes_clusters[cluster]\n",
    "            \n",
    "            for i in range(len(cluster_center_indexes)):\n",
    "                row_list = list(row)\n",
    "                row_list[cluster_center_indexes[i]] = new_coordinates[i]\n",
    "                row = tuple(row_list)\n",
    "        \n",
    "        return row\n",
    "            \n",
    "        \n",
    "        \n",
    "    return map_update_cluster\n",
    "\n",
    "def getRandomColor():\n",
    "    return '#{:06x}'.format(randint(0, 0xffffff))    \n",
    "    \n",
    "def drawKmeans(data, cluster_centers, colorlist):\n",
    "    datalist = data.collect()\n",
    "    plt.figure()\n",
    "    \n",
    "    dict_point = {}\n",
    "    \n",
    "    for point in datalist:\n",
    "        cluster = point[0]\n",
    "        pointdict = dict_point.get(cluster, {})\n",
    "        pointlist = pointdict.get(\"points\",[])\n",
    "        pointdict[\"color\"] = colorlist[cluster - 1]\n",
    "        \n",
    "        pointlist.append([point[1],point[2]])\n",
    "        pointdict[\"points\"] = pointlist\n",
    "        \n",
    "        dict_point[cluster] = pointdict\n",
    "    \n",
    "    #print(\"Dict point \", dict_point)\n",
    "    \n",
    "    for key in dict_point:\n",
    "        pointlist = dict_point[key][\"points\"]\n",
    "        color = dict_point[key][\"color\"]\n",
    "        \n",
    "        x = [item[0] for item in pointlist]\n",
    "        y = [item[1] for item in pointlist]\n",
    "        \n",
    "        #print(\"Color \", color)\n",
    "        #print(\"X \", x)\n",
    "        #print(\"Y \",y)\n",
    "        plt.scatter(x,y,color = color, marker=\"s\")\n",
    "        \n",
    "    x_clusters = [point[0] for point in cluster_centers]\n",
    "    y_clusters = [point[1] for point in cluster_centers]\n",
    "    \n",
    "    plt.plot(x_clusters, y_clusters, 'r^')\n",
    "    \n",
    "def kmeans(data, cluster_centers, num_iterations, num_features, visualize=False):\n",
    "    if visualize:\n",
    "        colorlist = [getRandomColor() for i in range(len(cluster_centers))]\n",
    "\n",
    "    num_clusters = len(cluster_centers)\n",
    "    #print(\"Num clusters :\", num_clusters, \"\\n\")\n",
    "    \n",
    "    #Cluster is first in the tuple\n",
    "    data = data.map(lambda x : (-1,) + x)\n",
    "    \n",
    "    #print(\"Data with empty cluster\", data.take(3), \"\\n\")\n",
    "    data = data.map(computeCluster(num_features, num_clusters))\n",
    "    \n",
    "    #print(\"Data with cluster\", data.take(3), \"\\n\")\n",
    "    \n",
    "    for it in range(num_iterations):\n",
    "        dataReduce = data.map(formatDataReduce(num_features, num_clusters))\n",
    "        #print(\"Data prepared for the reduce :\", dataReduce.take(3), \"\\n\")\n",
    "        \n",
    "        reduce = dataReduce.reduceByKey(reduceComputeCenter(num_features))\n",
    "        collect = reduce.collect()\n",
    "        #print(\"Collect : \", collect)\n",
    "        \n",
    "        #COMPUTE AVERAGE\n",
    "        reduce_list = []\n",
    "        \n",
    "        #tuple reduce : ( 1, ([2,3,4], 112))\n",
    "        for tuple_reduce in reduce.collect():\n",
    "            cluster = tuple_reduce[0]\n",
    "            \n",
    "            tuple_value = tuple_reduce[1]\n",
    "            \n",
    "            coord_list = tuple_value[0]\n",
    "            \n",
    "            num_point = tuple_value[1]\n",
    "            \n",
    "            for i in range(0,num_features):\n",
    "                coord_list[i] = coord_list[i] / num_point\n",
    "            \n",
    "            reduce_list.append( (cluster, coord_list) )\n",
    "        \n",
    "        #print(\"Result of reduce : \", reduce_list, \"\\n\")\n",
    "        \n",
    "        \n",
    "        #reduce list :  (1, [2,3,4])\n",
    "        data = data.map(updateClusterCoords(num_features, num_clusters, reduce_list))\n",
    "        #print(\"Result of map update coords :\", data.take(3), \"\\n\")\n",
    "        \n",
    "        data = data.map(computeCluster(num_features, num_clusters))\n",
    "        #print(\"Data with updated cluster\", data.take(3), \"\\n\")\n",
    "        \n",
    "        if visualize and (it %10 == 0):\n",
    "            cluster_centers = []\n",
    "            for tuple_reduce in reduce_list:\n",
    "                cluster_centers.append([tuple_reduce[1][0],tuple_reduce[1][1]])\n",
    "            \n",
    "        \n",
    "            drawKmeans(data = data, cluster_centers=cluster_centers, colorlist=colorlist)\n",
    "             \n",
    "        print(\"--------------\")\n",
    "\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***En utilisant cet algorithme avec une initialisation reduceMax, nous obtenons de bons résultats***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-573eb7d962f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster1_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtakeSample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's1' is not defined"
     ]
    }
   ],
   "source": [
    "cluster1_center = s1.takeSample(False, 1)[0]\n",
    "cluster_centers = [cluster1_center]\n",
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "\n",
    "\n",
    "s1_map = s1.map(lambda x : x + cluster1_center)\n",
    "s1_map_clustered, cluster_centers = initCluster(s1_map, num_features=2,num_clusters=15, reducer=reduceMax, cluster_centers=cluster_centers, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's1_map_clustered' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-fa3da3703457>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkmeans_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms1_map_clustered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcluster_centers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvisualize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 's1_map_clustered' is not defined"
     ]
    }
   ],
   "source": [
    "kmeans_result = kmeans(s1_map_clustered, cluster_centers, num_iterations=30, num_features=2, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***En utilisant cet algorithme avec une initialisation reduceMaxWithRandom, nous obtenons de moins bons résultats que l'initialisation reduceMax.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster1_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-0d3662f0afaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms1_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcluster1_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster1_center' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_centers = [cluster1_center]\n",
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "\n",
    "\n",
    "s1_map = s1.map(lambda x : x + cluster1_center)\n",
    "s1_map_clustered, cluster_centers = initCluster(s1_map, num_features=2,num_clusters=15, reducer=reduceMaxWithRandom, cluster_centers=cluster_centers, visualize=True)\n",
    "kmeans_result = kmeans(s1_map_clustered, cluster_centers, num_iterations=30, num_features=2, visualize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***En Initialisant avec k-means++ non optimisé pour être distribué correctement (mais l'algorithme exact), le résultat est correct mais en théorie il peut mettre du temps à tourner et à nous fournir les résultats finaux.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cluster1_center' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-7cda4907ec72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_centers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initial cluster center : \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcluster1_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0ms1_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcluster1_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cluster1_center' is not defined"
     ]
    }
   ],
   "source": [
    "cluster_centers = [cluster1_center]\n",
    "print(\"Initial cluster center : \",cluster1_center, \"\\n\")\n",
    "\n",
    "\n",
    "s1_map = s1.map(lambda x : x + cluster1_center)\n",
    "s1_map_clustered, cluster_centers = initCluster(s1_map, num_features=2,num_clusters=15, reducer=reduceMaxWithRandom, cluster_centers=cluster_centers, distributed=False, visualize=True)\n",
    "kmeans_result = kmeans(s1_map_clustered, cluster_centers, num_iterations=30, num_features=2, visualize=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
